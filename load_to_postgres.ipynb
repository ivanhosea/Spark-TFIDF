{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a79c229-e015-4f4e-bdd7-d200e671c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Read Config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('loadpg_config.properties')\n",
    "SPARK_MASTER = config.get('loadpg', 'spark_master')\n",
    "SPARK_DRIVER_HOST = config.get('loadpg', 'spark_driver_host')\n",
    "SPARK_DRIVER_BINDADDRES = config.get('loadpg', 'spark_driver_bindaddress')\n",
    "HDFS_URL = config.get('loadpg', 'hdfs_url')\n",
    "HADOOP_USER_NAME = config.get('loadpg', 'hadoop_user_name')\n",
    "PG_DRIVER_JAR_PATH = config.get('loadpg', 'pg_driver_jar_path')\n",
    "PG_IP = config.get('loadpg', 'pg_ip')\n",
    "PG_USER = config.get('loadpg', 'pg_user')\n",
    "PG_PASS = config.get('loadpg', 'pg_pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab49fe4-15ef-4bd8-8d9f-947c9d0e729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import udf, col\n",
    "import os\n",
    "os.environ[\"HADOOP_USER_NAME\"] = HADOOP_USER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4775b57-acbc-43d3-a070-c0201b2ab1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 21:38:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load_to_Postgres\") \\\n",
    "    .config(\"spark.master\", SPARK_MASTER) \\\n",
    "    .config(\"spark.driver.host\", SPARK_DRIVER_HOST) \\\n",
    "    .config(\"spark.driver.bindAddress\", SPARK_DRIVER_BINDADDRES) \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", f'hdfs://{HDFS_URL}/') \\\n",
    "    .config(\"spark.executor.memory\", \"2G\") \\\n",
    "    .config(\"spark.hadoop.parquet.block.size\", \"16777216\") \\\n",
    "    .config(\"spark.hadoop.dfs.blocksize\", \"16777216\") \\\n",
    "    .config(\"spark.hadoop.dfs.replication\", \"1\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.1\") \\\n",
    "    .config(\"spark.jars\", PG_DRIVER_JAR_PATH) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6cab5-ac46-47ca-9abc-755f30ba6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "hadoop = sc._jvm.org.apache.hadoop\n",
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration()\n",
    "conf.set(\"fs.defaultFS\", f'hdfs://{HDFS_URL}/')\n",
    "path = hadoop.fs.Path('/wiki_out5')\n",
    "\n",
    "for f in fs.get(conf).listStatus(path):\n",
    "    print(str(f.getPath()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e7148-1d7e-4b97-b6bf-1a37ad78b613",
   "metadata": {},
   "source": [
    "### Load Doc Details to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48d5068-dfbf-422c-816a-e3a0ecb1e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hdfs_input = f'hdfs://{HDFS_URL}/wiki2'\n",
    "df = spark.read.parquet(hdfs_input).select(col('id').alias('doc_id'), 'title', 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0acea9-d841-47ff-b90e-330f54037659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- doc_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbe68ef-0198-4898-891b-2dafacc8f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{PG_IP}:5432/db1\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"doc_det2\") \\\n",
    "    .option(\"user\", PG_USER) \\\n",
    "    .option(\"password\", PG_PASS) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac2205-e7e4-4fba-89c7-32b8dba7c6cb",
   "metadata": {},
   "source": [
    "### Load TF-IDF to Postgres\n",
    "Note: Ingesting parallelly like the one done in this section will result in the data not being sorted physically in the destination database despite the source being sorted already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f8c84-8f55-452e-b399-304d34993a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_input = f'hdfs://{HDFS_URL}/wiki_out5'\n",
    "df = spark.read.parquet(hdfs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701c8cda-71a6-4f13-b272-45fb09f9154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- term: string (nullable = true)\n",
      " |-- term_tfidf_array: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- doc_id: string (nullable = true)\n",
      " |    |    |-- tfidf: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf31fc1-d24c-405f-8fc6-8cdfcdba64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_parse_structarr(structarr):\n",
    "    l1 = []\n",
    "    for x in structarr:\n",
    "        l1.append(f'\"({x.doc_id},{x.tfidf})\"')\n",
    "    return '{' + ','.join(l1) + '}'\n",
    "\n",
    "udf_parse_structarr = udf(f_parse_structarr, StringType())\n",
    "\n",
    "df_string = df.select(col(\"term\"), udf_parse_structarr(\"term_tfidf_array\").alias('docs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23765f-1a21-4767-8ecd-2316ceca584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_string.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{PG_IP}:5432/db1\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"term_docs2\") \\\n",
    "    .option(\"user\", PG_USER) \\\n",
    "    .option(\"password\", PG_PASS) \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env2",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
